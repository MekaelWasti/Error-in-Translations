{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Config\n",
    "\n",
    "\n",
    "Note from Mekael\n",
    "- For my team, use a virutal environment to keep deployment operations clean and tidy\n",
    "- You can do this by running the following command in your terminal\n",
    "- `python -m venv error-in-translations`\n",
    "- Activate the environment by selecting it as your kernel for your jupyter notebook. If it doesn't work you will have to figure it out\n",
    "- pip install the packages in the requirements file in the root directory of this repo\n",
    "- `pip install -r requirements.txt`\n",
    "\n",
    "\n",
    "- **If you install new packages that are not included in the environment, please add it to the requirements file manually or generate a new requirements file with the following command in the terminal**\n",
    "- `pip freeze > requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openlanguagedata/flores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\mekae\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\mekae\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install sacrebleu\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import spacy\n",
    "import random\n",
    "import sacrebleu\n",
    "\n",
    "# Not working\n",
    "OPENAI_API_KEY = 'sk-proj-o9TONJi0MW2tSiDMhRkxT3BlbkFJkUr03XQ5IfUaxamV0e3k'\n",
    "\n",
    "# Mekael's Personal Key, not being shared\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Benchmark\n",
    "\n",
    "This benchmark sets a baseline and tests the translation precision & accuracy our POC pipeline, against the bare translation capabilities of OPENAI's CHATGPT 3.5 Turbo via their API.\n",
    "\n",
    "If our POC performs better than the stock GPT 3.5 Turbo, it means that our proposed method is valubale and worthwhile to implement. \n",
    "\n",
    "For baseline testing purposes, our POC makes use of custom GPT 3.5 API prompting as the translation model as well as the quality estimation model. These will be replaced with a more sophisticated custom LLM solution during actual implementation.\n",
    "\n",
    "We will be using Meta's Flores 200 dataset for this testing, and scores will be in the form of spBLEU and/or chrF++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset = \"../flores/floresp-v2.0-rc.2/dev/dev.eng_Latn\"\n",
    "\n",
    "# Rename the Column\n",
    "column_names = [\"Text Lines\"]\n",
    "\n",
    "# Read in the Flores Dataset English Latn\n",
    "df = pd.read_csv(english_dataset, delimiter = '\\t', header=None, names=column_names)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translations using our proposed method (POC) and stock GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add POC (no glossary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt with entity translation\n",
    "def prompt_generator(text, source_language, target_language):\n",
    "  prompt = f\"Translate the following text from {source_language} into {target_language}: {text}\\n\"\n",
    "  if terms == {}:\n",
    "    return prompt\n",
    "  prompt = translations + prompt\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Translation code, can be replaced by other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(prompt):\n",
    "\n",
    "    client = OpenAI(api_key= OPENAI_API_KEY,)\n",
    "    print(prompt)\n",
    "    response = client.chat.completions.create(\n",
    "      messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "          }],\n",
    "      model=\"gpt-3.5-turbo\",)\n",
    "    translation = response.choices[0].message.content.strip().split(\"\\n\")[0]\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_estimator(original_text, translated_text):\n",
    "  client = OpenAI(api_key= OPENAI_API_KEY,)\n",
    "  prompt = f\"Evaluate the quality estimation of the following source and translation sentence pairs by following a step-by-step process: \\\n",
    "    Step 1: Estimate the perplexity of the translated sentence.\\\n",
    "    Step 2: Determine the token-level similarity between the source and translatedsentences.\\\n",
    "    Step 3: Combine the results and classify the translation quality into one of the following categories:'No meaning preserved', 'Some meaning preserved, but not understandable', 'Some meaning preserved and understandable', 'Most meaningpreserved, minor issues',or 'Perfect translation'.\\\n",
    "    Source:{original_text}.Translation:{translated_text}\"\n",
    "  print(prompt)\n",
    "  response = client.chat.completions.create(\n",
    "    messages=[{\n",
    "          \"role\": \"user\",\n",
    "          \"content\": prompt,\n",
    "        }],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "  result = response.choices[0].message.content\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devtest folder will be used for this baseline testing as dev\n",
    "# may be more likely to appear in training data\n",
    "flores_dataset = \"../flores/floresp-v2.0-rc.2/devtest\"\n",
    "\n",
    "language_datasets = os.listdir(flores_dataset)\n",
    "\n",
    "\n",
    "# Randomly select source and target languages\n",
    "src_language_dataset = random.choice(language_datasets)\n",
    "targ_language_dataset = random.choice(language_datasets)\n",
    "\n",
    "# Assure that the source and target languages are not the same\n",
    "while targ_language_dataset == src_language_dataset:\n",
    "    targ_language_dataset = random.choice(language_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Source & Target Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_random_language_from_dataset():\n",
    "\n",
    "    # Pull a random line from source and target datasets\n",
    "\n",
    "    src_sentences = []\n",
    "    targ_sentences = []\n",
    "\n",
    "    def read_dataset(path):\n",
    "        with open(flores_dataset + \"/\" + path, 'r', encoding=\"utf-8\") as dataset_file:\n",
    "            lines = dataset_file.readlines()\n",
    "            total_lines = len(lines)\n",
    "        return lines, total_lines\n",
    "\n",
    "            \n",
    "    src_lines, total_lines = read_dataset(src_language_dataset)\n",
    "    targ_lines, total_lines = read_dataset(src_language_dataset)\n",
    "\n",
    "    selected_line_int = random.randint(1, total_lines)\n",
    "\n",
    "\n",
    "    selected_line_src = src_lines[selected_line_int - 1]\n",
    "    selected_line_targ = targ_lines[selected_line_int - 1]\n",
    "\n",
    "    # print(selected_line_int)\n",
    "    # print(selected_line_src)\n",
    "\n",
    "    # print(src_sentences)\n",
    "    # print(targ_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate source to target using our POC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate source to target using stock GPT 3.5 Turbo\n",
    "\n",
    "\n",
    "def translate_gpt_api():\n",
    "    prompt = f'Translate \"{selected_line_src}\" from the {src_language_dataset} language into the target language {targ_language_dataset}. Output only the translated sentence'\n",
    "    translation = \"\"\n",
    "\n",
    "    client = OpenAI(api_key= OPENAI_API_KEY,)\n",
    "    print(prompt)\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "            }],\n",
    "        model=\"gpt-3.5-turbo\",)\n",
    "        # model=\"gpt-4o\",)\n",
    "\n",
    "    hypothesized_translation = response.choices[0].message.content.strip().split(\"\\n\")[0]\n",
    "\n",
    "    # print(\"\\n\\nMANUAL CHECK LIST PARAMS\\n\\n\")\n",
    "\n",
    "    # print(src_language_dataset, src_sentences)\n",
    "    # print(targ_language_dataset, targ_sentences)\n",
    "\n",
    "\n",
    "    # print(f'TRANSLATION: {hypothesized_translation}')\n",
    "\n",
    "    return hypothesized_translation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute spBLEU & chrF++ Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are reference cells using the scores with english language sentence showing how it works. \n",
    "\n",
    "```hypothesized_translation``` can be just a string which contains the translation from our model\n",
    "\n",
    "```target_translation``` should be a list of strings (the score expects list of lists of strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sentences being fed to the score, ```hypothesized_translation``` and all strings in ```target_translation``` are all in the same language. Be careful with what each variable contains there are similar named variables in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(hypothesized_translation, target_translation):\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    \n",
    "    bleu_score  = sacrebleu.corpus_bleu([hypothesized_translation], [target_translation], tokenize=\"intl\")\n",
    "    print(\"spBLEU Score: \", bleu_score.score)\n",
    "\n",
    "    chr_score  = sacrebleu.corpus_chrf([hypothesized_translation], [target_translation])\n",
    "\n",
    "    print(\"chrF++ Score: \", chr_score.score)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(hypothesized_translation)\n",
    "\n",
    "    # return bleu_score, chr_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 3.5 Turbo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate \"Temaşekirina kulîlkê gilyazê ya wek hanamî tê zanîn, ji sedsala 8an ve bûye perçeyekî kultura Japonan.\n",
      "\" from the devtest.kmr_Latn language into the target language devtest.szl_Latn. Output only the translated sentence\n",
      "\n",
      "\n",
      "\n",
      "spBLEU Score:  30.26300230972924\n",
      "chrF++ Score:  55.88577814907521\n",
      "\n",
      "\n",
      "\n",
      "Temaśekíra kulílıkê gilyazê ya wek hanamî tê znane, ze sedsła 8an ve buo perćeyekí kultura Japonan.\n",
      "Temaşekirina kulîlkê gilyazê ya wek hanamî tê zanîn, ji sedsala 8an ve bûye perçeyekî kultura Japonan.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_random_language_from_dataset()\n",
    "\n",
    "hypothesized_translation = translate_gpt_api()\n",
    "target_translation = [selected_line_targ]\n",
    "\n",
    "get_scores(hypothesized_translation, target_translation)\n",
    "print(selected_line_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
