{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo Translation \n",
    "\n",
    "- Bulgarian written in the Cyrillic script\n",
    "\n",
    "    - Учените от медицинското училище в университета в Станфод обявиха в понеделник изобретяването на нов диагностичен инструмент, който може да сортира клетките по тип: малък печатен чип, който може да бъде произведен с помощта на стандартни мастилено-струйни принтери за вероятно около един американски цент всеки.\n",
    "\n",
    "- English\n",
    "    - On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRL = [\"Учените от медицинското училище в университета в Станфод обявиха в понеделник изобретяването на нов диагностичен инструмент, който може да сортира клетките по тип: малък печатен чип, който може да бъде произведен с помощта на стандартни мастилено-струйни принтери за вероятно около един американски цент всеки.\"]\n",
    "\n",
    "# Kinyarwanda is one of the lowest resrouce langauges in FLORES\n",
    "LRL = [\"Kuwa mbere, abahanga ba siyansi bo mu Ishuri rikuru ry’ubuvuzi rya kaminuza ya Stanford batangaje ko havumbuwe igikoresho gishya cyo gusuzuma gishobora gutandukanya ingirabuzima fatizo hashingiwe ku bwoko: agakoresho gato gacapwa, gashobora gukorwa hifashishijwe icapiro risanzwe rya inkjet mu buryo bushoboka ni hafi igiceri kimwe c'Amerika kuri buri kamwe.\"]\n",
    "HRL = [\"On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key= OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mekae\\Desktop\\CS\\ML-DL\\Projects\\Lee Lab\\Error-in-Translations\\error-in-translations\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NLLB Forward LRL to HRL (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mekae\\Desktop\\CS\\ML-DL\\Projects\\Lee Lab\\Error-in-Translations\\error-in-translations\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(source_text, source_lang, target_lang):\n",
    "\n",
    "    # Kinyarwanda Tokenizer\n",
    "    tokenizer.src_lang = source_lang\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(source_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate the translation according to target language specified\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[target_lang], max_length=30\n",
    "    )\n",
    "\n",
    "    # Decode the translated tokens for translated text\n",
    "    translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Monday, scientists at Stanford University Medical School announced the discovery of a new diagnostic tool that can differentiate stem cells by type: a\n"
     ]
    }
   ],
   "source": [
    "# source_lang = \"bul_Cyrl\"\n",
    "source_lang = \"kin_Latn\"\n",
    "target_lang = \"eng_Latn\"\n",
    "# source_text = \"Студентите от Медицинския факултет на Станфордския университет обявиха в понеделник изобретението на нов диагностичен инструмент, който може да сортира клетки по тип на малък печатен чип, който може да бъде произведен с помощта на стандартен принтер за тапети вероятно за около един американски цент на човек.\"\n",
    "source_text = \"Kuwa mbere, abahanga ba siyansi bo mu Ishuri rikuru ry’ubuvuzi rya kaminuza ya Stanford batangaje ko havumbuwe igikoresho gishya cyo gusuzuma gishobora gutandukanya ingirabuzima fatizo hashingiwe ku bwoko: agakoresho gato gacapwa, gashobora gukorwa hifashishijwe icapiro risanzwe rya inkjet mu buryo bushoboka ni hafi igiceri kimwe c'Amerika kuri buri kamwe.\"\n",
    "\n",
    "\n",
    "machine_translation = translate(source_text, source_lang, target_lang)\n",
    "\n",
    "print(machine_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NLLB Back-Translation HRL (English) back to LRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ku wa mbere, abahanga bo mu Ishuri ry'Ubuvuzi rya Kaminuza ya Stanford batangaje ko bavumbuye igikoresho\n"
     ]
    }
   ],
   "source": [
    "# source_lang = \"eng_Latn\"\n",
    "# target_lang = \"kin_Latn\"\n",
    "\n",
    "# target_lang = \"bul_Cyrl\"\n",
    "# source_text = hypothesis_one\n",
    "\n",
    "# back_translation = translate(source_text, source_lang, target_lang)\n",
    "\n",
    "\n",
    "def back_translation(source_text, source_lang, target_lang):\n",
    "    return translate(source_text, target_lang, source_lang)\n",
    "\n",
    "back_translation_text = back_translation(machine_translation, source_lang, target_lang)\n",
    "\n",
    "print(back_translation_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GPT for Translation Error Detection - Using all previous translation references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation(source_lang, target_lang, source_text, machine_translation, back_translation_text):\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    # model=\"gpt-4o-2024-05-13\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"text\": f'Identify the errors in translations from {source_lang} to {target_lang}. I have provided 3 references:\\n\\n1. 1 initial low resource sentence that got translated into \\n2. 1 English translation, and then back translated into \\n3. 1 reference sentence to compare with the initial input\\n\\nHere they are:\\n\\n 1. {source_text} \\n\\n2. {machine_translation} \\n\\n3. {back_translation_text} \\n\\nRemember to most weight on the first one as that is the initial input from the user\\n\\nList the translation errors only, in the english translation that resulted\\n',\n",
    "            \"type\": \"text\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"text\": \"1. \\\"Учените\\\" was translated as \\\"Students\\\" instead of the correct \\\"Scientists.\\\"\\n2. \\\"медицинското училище в университета в Станфод\\\" was translated as \\\"Stanford University School of Medicine,\\\" which misses the importance of the definitive article \\\"the\\\" before \\\"Stanford.\\\"\\n3. \\\"мастилено-струйни принтери за вероятно около един американски цент всеки\\\" was partially omitted. It should indicate that the diagnostic tool can be produced for around one American cent each using standard inkjet printers.\\n4. \\\"sort cells by a type of small printed\\\" is an incomplete clause and does not capture \\\"малък печатен чип\\\" as \\\"a small printed chip.\\\"\\n\\nOverall, the English translation missed key details and structure from the original sentence.\",\n",
    "            \"type\": \"text\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "  )\n",
    "\n",
    "  return response.choices[0].message.content, machine_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translation Error:\\n- \"Students\" was translated instead of \"Scientists.\"\\n- \"Discovery\" was missing in the translation.\\n- The translation was missing the details about the diagnostic tool being able to be produced for around one American cent each using standard inkjet printers.\\n- Incorrect translation of \"small printed chip\" as \"small printed.\"'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, machine_translation = evaluate_translation(source_lang, target_lang, source_text, machine_translation, back_translation_text)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[44m\u001b[97mK\u001b[0m\u001b[41m\u001b[97m\u001b[0m\u001b[42m\u001b[97m\u001b[0m\n",
      "\u001b[44m\u001b[97mOn Mond\u001b[0may, scientists at Stanford University Medica\u001b[41m\u001b[97ml School announced the discovery of a new diagnostic tool\u001b[0m that can differentiate stem cells by type: a\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from termcolor import colored\n",
    "\n",
    "def highlight_errors(original_text, translation_text, error_json):\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    with open(error_json, encoding='utf-8') as file:\n",
    "        errors = json.load(file)[\"errors\"]\n",
    "        # print(errors)\n",
    "    \n",
    "    indices_og = {}\n",
    "    indices_translation = {}\n",
    "\n",
    "    for error in errors:\n",
    "        # print(error)\n",
    "        \n",
    "        indices_og[error[\"start_index_orig\"]] = error[\"end_index_orig\"]\n",
    "        indices_translation[error[\"start_index_translation\"]] = error[\"end_index_translation\"]\n",
    "\n",
    "\n",
    "    indices_og = dict(sorted(indices_og.items()))\n",
    "    indices_translation = dict(sorted(indices_translation.items()))\n",
    "\n",
    "    # print(indices_og, indices_translation)\n",
    "    \n",
    "\n",
    "    highlight_errors_og = \"\"\n",
    "    colors = [\"on_blue\",\"on_red\",\"on_green\"]\n",
    "    count = 0 \n",
    "\n",
    "    # print(list(indices_og.items()))\n",
    "\n",
    "    \n",
    "    highlight_errors_translation = \"\"\n",
    "    count = 0 \n",
    "    prev = 0\n",
    "\n",
    "    for key,val in indices_og.items():\n",
    "\n",
    "        # if key <= prev or val <= prev:\n",
    "            # prev = min(prev,val,key)\n",
    "\n",
    "        # print()\n",
    "        highlight_errors_og += original_text[0][prev:key]\n",
    "        # print()\n",
    "\n",
    "        if key < val and prev < val:\n",
    "            highlight_errors_og += colored(original_text[0][key:val], \"white\", colors[count])\n",
    "        \n",
    "        # print(prev,key,val,)\n",
    "\n",
    "        count += 1\n",
    "        prev = val\n",
    "\n",
    "    highlight_errors_og += original_text[0][prev:]\n",
    "    \n",
    "    count = 0 \n",
    "    prev = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    # \n",
    "    # \n",
    "    # \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for key,val in indices_translation.items():\n",
    "\n",
    "        # if key <= prev or val <= prev:\n",
    "            # prev = min(prev,val,key)\n",
    "\n",
    "        # print()\n",
    "        highlight_errors_translation += translation_text[0][prev:key]\n",
    "        # print()\n",
    "\n",
    "        if key < val and prev < val:\n",
    "            highlight_errors_translation += colored(translation_text[0][key:val], \"white\", colors[count])\n",
    "        \n",
    "        # print(prev,key,val,)\n",
    "\n",
    "        count += 1\n",
    "        prev = val\n",
    "\n",
    "    highlight_errors_translation += translation_text[0][prev:]\n",
    "\n",
    "    \n",
    "\n",
    "    return highlight_errors_og, highlight_errors_translation\n",
    "\n",
    "og, translation = highlight_errors(source_text, [machine_translation], \"error_sample.json\")\n",
    "\n",
    "print(og)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students from Stanford University Medical School announced Monday the invention of a new diagnostic tool that can sort cells by type of small printed chip,\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Error Mapping Allignment Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_mapping_alignment():\n",
    "    # TODO: Add error mapping\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def compute_scores(hypothesis, reference):\n",
    "    bleu = sacrebleu.corpus_bleu([hypothesis], [reference])\n",
    "    chrf = sacrebleu.corpus_chrf([hypothesis], [reference])\n",
    "    print(f\"spBLEU score: {bleu.score}\")\n",
    "    print(f\"chrF score: {chrf.score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spBLEU score: 22.466891648632068\n",
      "chrF score: 47.44336565879224\n"
     ]
    }
   ],
   "source": [
    "compute_scores(hypothesis_one, HRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full FLORES Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading devtest.ace_Arab\n",
      "['\"کامو جينو نا تيكويه عمو ٤ بولن ڽڠ هانا ديابيتيس ڽڠ اوايجيه ساکيت ديابيتيس،\" غتامه لى غوبڽن. \\n', 'در. ايهود اور، ڤروفيسور کدوکترن بق يونيۏرسيتس دلهاوسي دي هليفاک\\u200cس، نوفا سکوتيا ڠون کڤالا ديۏيسي كلينيس ڠون علميه دري اسوسياسي ديابيتيس کانادا ݢڤئيڠت بهوا ڤنليتين ڽو منتوڠ لم ماس ڽڠ کفون.\\n', 'لݢى لادوم اورڠ چاروڠ لاءينجيه، غوبڽن راݢو ڤکوه ڤثاکيت ديابيتيس جوت ڤولي، سبب ڽڠ جيتومى ڽو هانا مسڠكوت دڠن اورڠ ڽڠ كا مڤثاکيت ديابيتيس جنيه ١.\\n', 'بق اورو سنين، سارا دانيوس، سيکريتاريس تتڤ کوميتى نوبل سسترا بق اكاديمي سويديا، ݢعمومکن ڠون بق مندوم اورڠ لم ماس ڤروݢرم راديو دي راديو سۏاريا دي سويديا کوميتى ڽن، هن جوت مسمبوڠ دڠن بوب ديلان ڠون لانسوڠ تنتڠ کمنڠن هديه نوبل ٢٠١٦ لم سسترا، كا ݢڤتيڠݢاي اوساها کى مسمبوڠ دڠن غوبڽن.\\n', 'دانيوس خن، \"جينو كامو هان مبوت سڤو. لون کا لون تليڤون ڠون لون کيريم ايمايل کى کولابوراتور ݢوبڽن ڽڠ ڤاليڠ رب ڠون لون تريموڠ باله ڽڠ ڤاليڠ روميه. جينو، ڽن کا سيڤ.\\n', 'سيݢوهلوم غوبڽن، سي.اي.او. ريڠ، جامي سيمينوف، ݢخن کى ڤراوسهاءن فون واتى بيل ڤنتو غوبڽن هان جوت مدڠو دري كدى غوبڽن لم ݢرسي غوبڽن.\\n', 'غوبڽن ڤݢوت بيل ڤنتو وايفاي، جخن لى غوبڽن.\\n', 'سيمينوف خن حسى مکت ݢاءيق لهوه ڤياسن غوبڽن بق تهون ٢٠١٣ لم ايڤيسودى شارک تاڠ\\u200cک لم ڤت ڤانل اچارا تولا بري دانا کى ستارتوڤ.\\n', 'بق اخى ٢٠١٧، سيمينوف توک بق سالورن تيليۏيسي بلنجا کيو.ۏي.سي.\\n', 'ريڠ ݢسلسو چيت ݢوݢتن ڠون ڤروسهاءن کأمنن لاون، أي.دي.تي کورڤوراشين.\\n']\n",
      "Reading devtest.ace_Latn\n",
      "['\"Kamöe jinoe na tikoih umu 4 buleuen nyang hana diabetes nyang awaijih saket diabetes,\" geutamah le gobnyan. \\n', 'Dr. Ehud Ur, profesor kedokteran bak Universitas Dalhousie di Halifax, Nova Scotia ngôn keupala divisi klinis ngôn ilmiah dari Asosiasi Diabetes Kanada geupeuingat bahwa penelitian nyoe mantöng lam masa nyang keuphôn.\\n', 'Lagèe ladom ureuëng caröng laenjih, gobnyan ragu peukeuh peunyaket diabetes jeut puléh, sabab nyang jiteumèe nyoe hana meusangkot deungôn ureuëng nyang ka meupeunyaket diabetes jeunèh 1.\\n', 'Bak uroe Senin, Sara Danius, sekretaris teutap Komite Nobel Sastra bak Akademi Swedia, geuumumkan ngôn bak mandum ureuëng lam masa program radio di Radio Sveriges di Swedia komite nyan, han jeuet meusambông deungôn Bob Dylan ngôn lansông teuntang keumenangan Hadiah Nobel 2016 lam Sastra, ka geupeutinggai usaha keu meusambông deungôn gobnyan.\\n', 'Danius kheun, \"Jinoe kamöe hana meubuet sapeue. Lôn ka lôn talipun ngön lôn kirém email keu kolaborator gobnyan nyan paléng rab ngôn lôn teurimöng balah nyang paléng ruméh. Jinoe, nyan ka sep.\\n', 'Sigohlom gobnyan, CEO Ring, Jamie Siminoff, geukheun keu perusahaan phôn watèe bel pinto gobnyan han jeuet meudeungö dari keudèe gobnyan lam garasi gobnyan.\\n', 'Gobnyan peugöt bel pinto WiFi, jikheun lé gobnyan.\\n', 'Siminoff kheun hasé meukat geuék ‘oh lheuh piasan gobnyan bak thôn 2013 lam episode Shark Tank lam pat panel acara tula bri dana keu startup.\\n', 'Bak akhée 2017, Siminoff teuka bak saluran televisi beulanja QVC.\\n', 'Ring geuseuleuso cit gugatan ngôn peurusahaan keamanan lawan, ADT Corporation.\\n']\n",
      "Reading devtest.acm_Arab\n",
      "['أضاف بكلامه وكَال، \"عدنا هسه فيران عمرها 4 أشهر، وكانت سابقاً مصابة بمرض السكر، بس هسه تعافت منه.\\n', 'نبهنا الدكتور إيهود أور -أستاذ الطب بجامعة دالهوزي بمنطقة هاليفاكس، نوفا سكوتيا ورئيس الشعبة الطبية والعلمية بالجمعية الكندية لأمراض السكري- إن البحث بعده  بمراحله الأولى.\\n', 'مثل بعض الخبراء الآخرين هو متشكك من امكانية علاج مرض السكر، مع ملاحظة أنو هذي النتائج ما إلها علاقة بالناس اللي يعانون من السكر من النوع الأول.\\n', ' يوم الاثنين، كَالت سارة دانيوس -السكرتيرة الدائمية للجنة نوبل للأداب بالأكاديمية السويدية- كَدام الناس خلال برنامج على راديو السويد بالسويد، أنو اللجنة ما كدرت توصل مباشرة لبوب ديلان علمود الفوز بجائزة نوبل للأدب سنة 2016- وتخلت عن جهودها  بالوصول إله.\\n', 'كَالت دانيوس \" ما راح نسوي أي شي. أني اتصلت ودزيت رسائل بريد إلكتروني لأقرب المتعاونين وياه وحلصت على ردود كلش لطيفة. هذا  كاببالوقت الحالي طبعاً\".\\n', '  قبل مدة، لاحظ جيمي سيمينوف، الرئيس التنفيذي لشركة Ring، أنو الشركة تأسست لمن ما  كان  جرس بابه مسموع من دكانه اللي صاير بكراج بيته.\\n', 'كَال إنه صنع جرس للباب يشتغل بتقنية الواي فاي.\\n', 'كَال سيمينوف إن المبيعات ارتفعت بعد ما طلع بالتلفزيون سنة2013 بحلقة برنامج شارك تانك، ورفضت لجنة العرض تمويل الشركة البادئة.\\n', 'بنهاية عام 2017، طلع \"سيمينوف\" على قناة التسوق التلفزيونية كيو بسي.\\n', ' وقامت رينج بتسوية دعوى قضائية وية شركة الأمن المنافسة، شركة آي دي تي.\\n']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../flores/floresp-v2.0-rc.2/devtest\"\n",
    "english_data_path = \"flores/floresp-v2.0-rc.2/devtest/devtest.eng_Latn\"\n",
    "\n",
    "lines = []\n",
    "\n",
    "\n",
    "'''\n",
    "Want to test translations from first n language datasets, \n",
    "against their English Machine translations through the pipeline \n",
    "\n",
    "First k lines from the first n language datasets\n",
    "'''\n",
    "\n",
    "n = 3\n",
    "\n",
    "for filename in os.listdir(dataset_path):\n",
    "    print(\"Reading\", filename)\n",
    "    \n",
    "    with open(f'{dataset_path}/{filename}', encoding='utf-8') as dataset:\n",
    "\n",
    "        lines = [next(dataset) for _ in range(10)]\n",
    "\n",
    "        print(lines)\n",
    " \n",
    "    n -= 1\n",
    "    if n == 0:\n",
    "        break\n",
    "\n",
    "\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Agent Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleChain\n",
    "\n",
    "def pipeline(text):\n",
    "    # 1: LRL to HRL Forward Translation\n",
    "    translated_text = translate(text, source_lang=\"LRL_code\", target_lang=\"en\")\n",
    "    \n",
    "    # 2: Back Translation\n",
    "    back_translated_text = translate(translated_text, source_lang=\"en\", target_lang=\"LRL_code\")\n",
    "    \n",
    "    # Step 3: Error Identification & Classification\n",
    "    errors = classify_errors(back_translated_text)\n",
    "    \n",
    "    # Step 4: Error Mapping Alignment\n",
    "    error_mapping_alignment()\n",
    "    \n",
    "    return {\n",
    "        \"original_text\": text,\n",
    "        \"translated_text\": translated_text,\n",
    "        \"back_translated_text\": back_translated_text,\n",
    "        \"errors\": errors\n",
    "    }\n",
    "\n",
    "result = pipeline(\"Didn't get here being careful \")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGH\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NllbTokenizerFast' object has no attribute 'get_lang_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msrc_lang \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m encoded_zh \u001b[38;5;241m=\u001b[39m tokenizer(chinese_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_zh, forced_bos_token_id\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lang_id\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      8\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_tokens, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NllbTokenizerFast' object has no attribute 'get_lang_id'"
     ]
    }
   ],
   "source": [
    "# chinese_text = \"生活就像一盒巧克力。\"\n",
    "chinese_text = \"忘记了我在打扰的时间\"\n",
    "\n",
    "# translate Chinese to English\n",
    "tokenizer.src_lang = \"zh\"\n",
    "encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "error-in-translations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
